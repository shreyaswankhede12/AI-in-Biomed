
\section{Introduction}
Emotions are essential aspects of human life, influencing our daily choices, understanding of the world, social interactions, and cognitive abilities. Consequently, emotion recognition is crucial in therapeutic approaches such as Cognitive Behavioral Therapy (CBT), Emotion Regulation Therapy (ERT), and Emotion-Focused Therapy (EFT). It also plays a vital role in assessing medical interventions for mental health conditions such as Generalized Anxiety Disorder (GAD) and Depression. Hence, there has been growing interest among researchers in leveraging Artificial Intelligence to detect and understand human emotions, especially for applications in CBT and EFT.

Electroencephalography (EEG) is a sophisticated and non-invasive neuroimaging technique that measures and records the electrical activity of the brain. By placing multiple electrodes on the scalp, EEG captures and amplifies the electrical signals produced by the brain's neurons, providing invaluable insights into its function and activity. One of the significant advantages of EEG is its high temporal resolution, allowing researchers and clinicians to observe and analyze rapid changes in brain states at a sub-second level. This temporal precision is particularly beneficial in studying various cognitive processes, emotional responses, and neurological disorders.

In recent years, EEG technology has seen remarkable advancements, especially with the integration of machine learning and signal processing techniques. These technological innovations have enabled the development of sophisticated Brain-Computer Interface (BCI) systems that can interpret and translate EEG signals into actionable insights. BCI systems equipped with EEG-based emotion detection algorithms can identify and classify human emotions, offering new avenues for understanding emotional states and enhancing human-computer interaction. Moreover, EEG-based BCI systems have found applications beyond academic research and clinical settings. They are increasingly being used in various industries, including healthcare, gaming, and assistive technology. In healthcare, EEG helps in diagnosing and monitoring neurological conditions such as epilepsy, sleep disorders, and cognitive impairments. In gaming and virtual reality, EEG-based BCI systems offer immersive and interactive experiences by adapting gameplay in real-time based on the player's cognitive and emotional states. Additionally, in assistive technology, EEG allows individuals with severe motor impairments to communicate and interact with their environment using brain signals, improving their quality of life. Numerous researchers such as Zheng et al., Li et al. and others have come up with various ways for using EEG-BCIs for emotion recognition such as using a a discriminative graph regularized extreme learning machine to investigate stable patterns over time from the differential entropy (DE) features of emotional EEG, or utilizing phase-locking value to construct emotion-related brain networks with multiple feature fusion to detect emotions from EEG.

Recently, deep learning-based methods have shown promising results
in the BCI domain, such as motor imagery classification , emotion recognition
, and mental-task classification . Yang
et al.  designed a hierarchical network structure to perform emotion classification, proposing sub-network nodes
to enhance the performance. Li et al. constructed EEG
into 2D images and proposed a Hierarchical Convolutional
Neural Networks (HCNN) to extract the spatial patterns of
the EEG. Li et al.  applied 18 kinds of linear and nonlinear features to solve the cross-subject emotion recognition problems, achieving 59.06% and 83.33% on two public
datasets. Zhang et al.  utilized recurrent neural networks
(RNN) to learn the temporal-spatial information from the
DE features of EEG for emotion recognition.

Although many
machine learning methods have been proposed for emotion
recognition, most of them highly rely on hand-crafted features. Convolutional Neural Networks (CNNs) have demonstrated significant potential in Brain-Computer Interface (BCI) applications, particularly when learning directly from EEG data. Schirrmeister et al. introduced DeepConvNet and ShallowConvNet, which are specialized CNN architectures designed to process EEG data. These networks combine feature extraction and classification through a two-stage convolution layer that considers both spatial and temporal input. Similarly, Lawhern et al. developed EEGNet, utilizing a depth-wise convolution kernel with a size of (n, 1) to capture spatial information, where 'n' represents the number of channels. These networks employ single-scale 1D convolutional kernels across both time and channel dimensions to extract both temporal and spatial information from EEG data.

To effectively capture the temporal-spatial dynamics in EEG for emotion recognition, it's essential to consider various neurophysiological signatures. In the temporal dimension, EEG signals encompass rich brain activity across different frequency bands. Given the dynamic and nonstationary nature of EEG, we believe that using a fixed-size temporal kernel may not adequately capture the diverse neural processes underlying emotions, which operate at varying time scales and durations. In terms of spatial considerations, emotions in the brain often elicit asymmetric responses between the right and left hemispheres. Therefore, we posit that a uniform spatial kernel might not be optimal for accurately extracting the unique asymmetric EEG patterns associated with emotional processes.

To address these issues, authors Yi Ding, et al. proposed the TSception model, a multi-scale temporal-spatial convolutional neural network designed for EEG-based emotion recognition. Unlike traditional methods that rely on manually extracted features, TSception directly processes EEG signals, making it an end-to-end deep learning approach that requires minimal domain-specific knowledge. The model comprised of a dynamic temporal layer, a asymmetric hemisphere layer and a high level fusion layer. The dynamic temporal layer, inspired by GoogleNet's inception block, employs different scaled convolutional kernels to extract richer time-frequency representations from EEG, moving beyond single-sized temporal CNN kernels. The asymmetric spatial layer captures the asymmetric patterns between the brain's right and left hemispheres, enhancing the network's ability to distinguish emotion-specific information. To streamline the network and facilitate online applications, the authors included a high-level fusion layer that combines hemisphere-global representations.

In this paper, we introduce an extended version of TSception, the TSception-2 which comprises of an additional Long Short-Term Memory (LSTM) layer to further enhance the network's capability in capturing complex sequebtial temporal dynamics and spatial asymmetries from EEG data. We evaluated the performance of the extended TSception-2 on the publicly available benchmark dataset: the Database for Emotion Analysis using Physiological signals (DEAP). Comparative studies were conducted against the original TSeption. Our results demonstrate that the extended TSception-2 consistently outperforms the original TSeption-2 in terms of accuracy and F1 score. A comprehensive analysis, including extensive ablation studies and interpretability experiments using saliency maps, was conducted to understand the contributions of each module and the areas of EEG data that the network primarily focuses on, notably the frontal, temporal, and parietal regions associated with emotional processes.

The major contributions of this extended work include the introduction of an LSTM layer to enhance temporal sequence learning in TSception and the comprehensive evaluation and interpretability analysis conducted to understand the network's functionality and significance.

